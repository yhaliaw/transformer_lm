# transformer_lm (Outdated/Archived)
Implementations of various transformer-based language models.

The transformer models and components are implemented using PyTorch. 
Supports a wide variety of features such as, floating point 16 training, many learning rate schedules, adaptive input, adaptive softmax, etc.

This is the old code base before migration to using Fairseq.
